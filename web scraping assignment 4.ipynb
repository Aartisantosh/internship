{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba32b7d",
   "metadata": {},
   "source": [
    "• Read all the problem statements, notes carefully and scrape the required data using any web scraping tool of\n",
    "your choice.\n",
    "• You have to handle commonly occurring EXCEPTIONS by using exception handling programing. To get\n",
    "information about selenium Exceptions. You may visit following links:\n",
    "1. https://selenium-python.readthedocs.io/api.html\n",
    "2. https://www.guru99.com/exception-handling-selenium.html\n",
    "3. https://stackoverflow.com/questions/38022658/selenium-python-handling-no-such-elementexception/38023345\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29243ea6",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9f2ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7d56b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets connect with web driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5baebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')\n",
    "time.sleep(3)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8bdbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "Views=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22267627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Rank\n",
    "rank=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr')\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n",
    "\n",
    "#Scraping Name\n",
    "name=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td[1]')\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping Artist\n",
    "artist=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td[2]')\n",
    "for i in artist:\n",
    "    Artist.append(i.text)\n",
    "    \n",
    "    \n",
    "#Scraping Views\n",
    "views=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td[3]')\n",
    "for i in views:\n",
    "    Views.append(i.text)\n",
    "    \n",
    "    \n",
    "#Scraping Upload_date\n",
    "upload_date=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td[4]')\n",
    "for i in upload_date:\n",
    "    Upload_date.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4198832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'RANK':Rank,'NAME':Name,'ARTIST':Artist,'UPDOADED DATE':Upload_date,'VIEWS':Views})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6d3961b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>NAME</th>\n",
       "      <th>ARTIST</th>\n",
       "      <th>UPDOADED DATE</th>\n",
       "      <th>VIEWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[6] Pinkfong Baby Shark - Ki...</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>14.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Despacito\"[9] Luis Fonsi 8.38 January 12, 201...</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Johny Johny Yes Papa\"[17] LooLoo Kids - Nurse...</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[17]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bath Song\"[18] Cocomelon - Nursery Rhymes 6.6...</td>\n",
       "      <td>\"Bath Song\"[18]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Shape of You\"[19] Ed Sheeran 6.20 January 30,...</td>\n",
       "      <td>\"Shape of You\"[19]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"See You Again\"[22] Wiz Khalifa 6.17 April 6, ...</td>\n",
       "      <td>\"See You Again\"[22]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>6.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"Wheels on the Bus\"[27] Cocomelon - Nursery Rh...</td>\n",
       "      <td>\"Wheels on the Bus\"[27]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Phonics Song with Two Words\"[28] ChuChu TV Nu...</td>\n",
       "      <td>\"Phonics Song with Two Words\"[28]</td>\n",
       "      <td>ChuChu TV Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Uptown Funk\"[29] Mark Ronson 5.15 November 19...</td>\n",
       "      <td>\"Uptown Funk\"[29]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[30...</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[30]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>5.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Gangnam Style\"[31] Psy 5.05 July 15, 2012 [E]</td>\n",
       "      <td>\"Gangnam Style\"[31]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>5.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[36]...</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[36]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Dame Tu Cosita\"[37] Ultra Records 4.55 April ...</td>\n",
       "      <td>\"Dame Tu Cosita\"[37]</td>\n",
       "      <td>Ultra Records</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Axel F\"[38] Crazy Frog 4.34 June 16, 2009</td>\n",
       "      <td>\"Axel F\"[38]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Sugar\"[39] Maroon 5 4.00 January 14, 2015</td>\n",
       "      <td>\"Sugar\"[39]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Counting Stars\"[40] OneRepublic 3.97 May 31, ...</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Baa Baa Black Sheep\"[41] Cocomelon - Nursery ...</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"Roar\"[42] Katy Perry 3.96 September 5, 2013</td>\n",
       "      <td>\"Roar\"[42]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Lakdi Ki Kathi\"[43] Jingle Toons 3.91 June 14...</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[43]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[44] Shakira...</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[44]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"Sorry\"[45] Justin Bieber 3.77 October 22, 2015</td>\n",
       "      <td>\"Sorry\"[45]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"Thinking Out Loud\"[46] Ed Sheeran 3.73 Octobe...</td>\n",
       "      <td>\"Thinking Out Loud\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47] Kiddie...</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47]</td>\n",
       "      <td>Kiddiestv Hindi - Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Shree Hanuman Chalisa\"[48] T-Series Bhakti Sa...</td>\n",
       "      <td>\"Shree Hanuman Chalisa\"[48]</td>\n",
       "      <td>T-Series Bhakti Sagar</td>\n",
       "      <td>May 10, 2011</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"Dark Horse\"[49] Katy Perry 3.67 February 20, ...</td>\n",
       "      <td>\"Dark Horse\"[49]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"Perfect\"[50] Ed Sheeran 3.67 November 9, 2017</td>\n",
       "      <td>\"Perfect\"[50]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Let Her Go\"[51] Passenger 3.61 July 25, 2012</td>\n",
       "      <td>\"Let Her Go\"[51]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"Faded\"[52] Alan Walker 3.59 December 3, 2015</td>\n",
       "      <td>\"Faded\"[52]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"Girls Like You\"[53] Maroon 5 3.56 May 31, 2018</td>\n",
       "      <td>\"Girls Like You\"[53]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Lean On\"[54] Major Lazer Official 3.55 March ...</td>\n",
       "      <td>\"Lean On\"[54]</td>\n",
       "      <td>Major Lazer Official</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 RANK  \\\n",
       "0   \"Baby Shark Dance\"[6] Pinkfong Baby Shark - Ki...   \n",
       "1   \"Despacito\"[9] Luis Fonsi 8.38 January 12, 201...   \n",
       "2   \"Johny Johny Yes Papa\"[17] LooLoo Kids - Nurse...   \n",
       "3   \"Bath Song\"[18] Cocomelon - Nursery Rhymes 6.6...   \n",
       "4   \"Shape of You\"[19] Ed Sheeran 6.20 January 30,...   \n",
       "5   \"See You Again\"[22] Wiz Khalifa 6.17 April 6, ...   \n",
       "6   \"Wheels on the Bus\"[27] Cocomelon - Nursery Rh...   \n",
       "7   \"Phonics Song with Two Words\"[28] ChuChu TV Nu...   \n",
       "8   \"Uptown Funk\"[29] Mark Ronson 5.15 November 19...   \n",
       "9   \"Learning Colors – Colorful Eggs on a Farm\"[30...   \n",
       "10     \"Gangnam Style\"[31] Psy 5.05 July 15, 2012 [E]   \n",
       "11  \"Masha and the Bear – Recipe for Disaster\"[36]...   \n",
       "12  \"Dame Tu Cosita\"[37] Ultra Records 4.55 April ...   \n",
       "13         \"Axel F\"[38] Crazy Frog 4.34 June 16, 2009   \n",
       "14         \"Sugar\"[39] Maroon 5 4.00 January 14, 2015   \n",
       "15  \"Counting Stars\"[40] OneRepublic 3.97 May 31, ...   \n",
       "16  \"Baa Baa Black Sheep\"[41] Cocomelon - Nursery ...   \n",
       "17       \"Roar\"[42] Katy Perry 3.96 September 5, 2013   \n",
       "18  \"Lakdi Ki Kathi\"[43] Jingle Toons 3.91 June 14...   \n",
       "19  \"Waka Waka (This Time for Africa)\"[44] Shakira...   \n",
       "20    \"Sorry\"[45] Justin Bieber 3.77 October 22, 2015   \n",
       "21  \"Thinking Out Loud\"[46] Ed Sheeran 3.73 Octobe...   \n",
       "22  \"Humpty the train on a fruits ride\"[47] Kiddie...   \n",
       "23  \"Shree Hanuman Chalisa\"[48] T-Series Bhakti Sa...   \n",
       "24  \"Dark Horse\"[49] Katy Perry 3.67 February 20, ...   \n",
       "25     \"Perfect\"[50] Ed Sheeran 3.67 November 9, 2017   \n",
       "26      \"Let Her Go\"[51] Passenger 3.61 July 25, 2012   \n",
       "27      \"Faded\"[52] Alan Walker 3.59 December 3, 2015   \n",
       "28    \"Girls Like You\"[53] Maroon 5 3.56 May 31, 2018   \n",
       "29  \"Lean On\"[54] Major Lazer Official 3.55 March ...   \n",
       "\n",
       "                                               NAME  \\\n",
       "0                             \"Baby Shark Dance\"[6]   \n",
       "1                                    \"Despacito\"[9]   \n",
       "2                        \"Johny Johny Yes Papa\"[17]   \n",
       "3                                   \"Bath Song\"[18]   \n",
       "4                                \"Shape of You\"[19]   \n",
       "5                               \"See You Again\"[22]   \n",
       "6                           \"Wheels on the Bus\"[27]   \n",
       "7                 \"Phonics Song with Two Words\"[28]   \n",
       "8                                 \"Uptown Funk\"[29]   \n",
       "9   \"Learning Colors – Colorful Eggs on a Farm\"[30]   \n",
       "10                              \"Gangnam Style\"[31]   \n",
       "11   \"Masha and the Bear – Recipe for Disaster\"[36]   \n",
       "12                             \"Dame Tu Cosita\"[37]   \n",
       "13                                     \"Axel F\"[38]   \n",
       "14                                      \"Sugar\"[39]   \n",
       "15                             \"Counting Stars\"[40]   \n",
       "16                        \"Baa Baa Black Sheep\"[41]   \n",
       "17                                       \"Roar\"[42]   \n",
       "18                             \"Lakdi Ki Kathi\"[43]   \n",
       "19           \"Waka Waka (This Time for Africa)\"[44]   \n",
       "20                                      \"Sorry\"[45]   \n",
       "21                          \"Thinking Out Loud\"[46]   \n",
       "22          \"Humpty the train on a fruits ride\"[47]   \n",
       "23                      \"Shree Hanuman Chalisa\"[48]   \n",
       "24                                 \"Dark Horse\"[49]   \n",
       "25                                    \"Perfect\"[50]   \n",
       "26                                 \"Let Her Go\"[51]   \n",
       "27                                      \"Faded\"[52]   \n",
       "28                             \"Girls Like You\"[53]   \n",
       "29                                    \"Lean On\"[54]   \n",
       "\n",
       "                                               ARTIST      UPDOADED DATE  \\\n",
       "0         Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "1                                          Luis Fonsi   January 12, 2017   \n",
       "2   LooLoo Kids - Nursery Rhymes and Children's Songs    October 8, 2016   \n",
       "3                          Cocomelon - Nursery Rhymes        May 2, 2018   \n",
       "4                                          Ed Sheeran   January 30, 2017   \n",
       "5                                         Wiz Khalifa      April 6, 2015   \n",
       "6                          Cocomelon - Nursery Rhymes       May 24, 2018   \n",
       "7               ChuChu TV Nursery Rhymes & Kids Songs      March 6, 2014   \n",
       "8                                         Mark Ronson  November 19, 2014   \n",
       "9                                         Miroshka TV  February 27, 2018   \n",
       "10                                                Psy      July 15, 2012   \n",
       "11                                         Get Movies   January 31, 2012   \n",
       "12                                      Ultra Records      April 5, 2018   \n",
       "13                                         Crazy Frog      June 16, 2009   \n",
       "14                                           Maroon 5   January 14, 2015   \n",
       "15                                        OneRepublic       May 31, 2013   \n",
       "16                         Cocomelon - Nursery Rhymes      June 25, 2018   \n",
       "17                                         Katy Perry  September 5, 2013   \n",
       "18                                       Jingle Toons      June 14, 2018   \n",
       "19                                            Shakira       June 4, 2010   \n",
       "20                                      Justin Bieber   October 22, 2015   \n",
       "21                                         Ed Sheeran    October 7, 2014   \n",
       "22      Kiddiestv Hindi - Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "23                              T-Series Bhakti Sagar       May 10, 2011   \n",
       "24                                         Katy Perry  February 20, 2014   \n",
       "25                                         Ed Sheeran   November 9, 2017   \n",
       "26                                          Passenger      July 25, 2012   \n",
       "27                                        Alan Walker   December 3, 2015   \n",
       "28                                           Maroon 5       May 31, 2018   \n",
       "29                               Major Lazer Official     March 22, 2015   \n",
       "\n",
       "    VIEWS  \n",
       "0   14.09  \n",
       "1    8.38  \n",
       "2    6.87  \n",
       "3    6.62  \n",
       "4    6.20  \n",
       "5    6.17  \n",
       "6    5.88  \n",
       "7    5.70  \n",
       "8    5.15  \n",
       "9    5.07  \n",
       "10   5.05  \n",
       "11   4.58  \n",
       "12   4.55  \n",
       "13   4.34  \n",
       "14   4.00  \n",
       "15   3.97  \n",
       "16   3.96  \n",
       "17   3.96  \n",
       "18   3.91  \n",
       "19   3.85  \n",
       "20   3.77  \n",
       "21   3.73  \n",
       "22   3.73  \n",
       "23   3.69  \n",
       "24   3.67  \n",
       "25   3.67  \n",
       "26   3.61  \n",
       "27   3.59  \n",
       "28   3.56  \n",
       "29   3.55  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c66c3",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497c5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a213e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n",
    "time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d547b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.bcci.tv/\")\n",
    "time.sleep(4)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5320f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on international\n",
    "international=driver.find_element(By.XPATH,'//ul[@class=\"list-unstyled ps-0 mb-0\"]/li/a[2]')\n",
    "international.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026fb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on fixtures\n",
    "fixtures= driver.find_element(By.XPATH,'//div[@class=\"imw-tabs international-tabs\"]/a[2]')\n",
    "fixtures.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59643ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Empty list\n",
    "Match_title=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34070d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Match_title\n",
    "match_title= driver.find_elements(By.XPATH,'//div[@class=\"match-card-top\"]/div/div/span[1]')\n",
    "for i in match_title:\n",
    "    Match_title.append(i.text)\n",
    "    \n",
    "#Scraping Series\n",
    "series= driver.find_elements(By.XPATH,'//div[@class=\"match-card-top\"]/div/div/span[2]')\n",
    "for i in series:\n",
    "    Series.append(i.text)\n",
    "    \n",
    "#Scraping Place\n",
    "place= driver.find_elements(By.XPATH,'//div[@class=\"match-card-top\"]/div/h5')\n",
    "for i in place:\n",
    "    Place.append(i.text)   \n",
    "    \n",
    "#Scraping Date\n",
    "date= driver.find_elements(By.XPATH,'//div[@class=\"match-date-info\"]/div[1]')\n",
    "for i in date:\n",
    "    Date.append(i.text)   \n",
    "    \n",
    "#Scraping Time\n",
    "time= driver.find_elements(By.XPATH,'//div[@class=\"match-date-info\"]/div[2]')\n",
    "for i in time:\n",
    "    Time.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d84351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'MATCH':Match_title,'SERIES':Series,'PLACE':Place,'DATE':Date,'TIME':Time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bda562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MATCH</th>\n",
       "      <th>SERIES</th>\n",
       "      <th>PLACE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4th Test</td>\n",
       "      <td>Men</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>23 FEBRUARY, 2024</td>\n",
       "      <td>9:30 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5th Test</td>\n",
       "      <td>Men</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>7 MARCH, 2024</td>\n",
       "      <td>9:30 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>Men</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>6 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>Men</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>7 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>Men</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>10 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>Men</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>13 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>Men</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>14 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MATCH SERIES                          PLACE               DATE  \\\n",
       "0  4th Test    Men  ENGLAND TOUR OF INDIA 2023-24  23 FEBRUARY, 2024   \n",
       "1  5th Test    Men  ENGLAND TOUR OF INDIA 2023-24      7 MARCH, 2024   \n",
       "2  1st T20I    Men    INDIA TOUR OF ZIMBABWE 2024       6 JULY, 2024   \n",
       "3  2nd T20I    Men    INDIA TOUR OF ZIMBABWE 2024       7 JULY, 2024   \n",
       "4  3rd T20I    Men    INDIA TOUR OF ZIMBABWE 2024      10 JULY, 2024   \n",
       "5  4th T20I    Men    INDIA TOUR OF ZIMBABWE 2024      13 JULY, 2024   \n",
       "6  5th T20I    Men    INDIA TOUR OF ZIMBABWE 2024      14 JULY, 2024   \n",
       "\n",
       "          TIME  \n",
       "0  9:30 AM IST  \n",
       "1  9:30 AM IST  \n",
       "2  8:00 PM IST  \n",
       "3  8:00 PM IST  \n",
       "4  8:00 PM IST  \n",
       "5  8:00 PM IST  \n",
       "6  8:00 PM IST  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4045517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "661ef38c",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37878321",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35e0ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n",
    "time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a8fde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://statisticstimes.com/\")\n",
    "time.sleep(2)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "45d6a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click Economy\n",
    "Economy=driver.find_element(By.XPATH,'//div[@class=\"dropdown\"][2]/button')\n",
    "Economy.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "90f79088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click India\n",
    "India= driver.find_element(By.XPATH,'//div[@class=\"dropdown\"][2]/div/a[3]')\n",
    "India.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec84985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on India GPD\n",
    "GDP=driver.find_element(By.XPATH,'//ul[@style=\"list-style-type:none;margin-left:20px;\"]/li[1]/a')\n",
    "GDP.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30d71a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8f4e3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[1]'):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "# scraping State\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[2]'):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[3]'):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[4]'):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "    \n",
    "# scraping Share (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[5]'):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "    \n",
    "# scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[6]'):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"_\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf58c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank),len(State),len( GSDP1),len(GSDP2),len(Share),len(GDP_billion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "856f6b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>STATE</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share (18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>3,108,022</td>\n",
       "      <td>13.24%</td>\n",
       "      <td>417.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>2,364,514</td>\n",
       "      <td>2,071,286</td>\n",
       "      <td>8.82%</td>\n",
       "      <td>278.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>2,257,575</td>\n",
       "      <td>1,974,532</td>\n",
       "      <td>8.41%</td>\n",
       "      <td>265.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>2,241,368</td>\n",
       "      <td>1,962,725</td>\n",
       "      <td>8.36%</td>\n",
       "      <td>263.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,937,066</td>\n",
       "      <td>8.25%</td>\n",
       "      <td>259.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,554,992</td>\n",
       "      <td>1,363,926</td>\n",
       "      <td>5.81%</td>\n",
       "      <td>183.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,413,620</td>\n",
       "      <td>1,218,193</td>\n",
       "      <td>5.19%</td>\n",
       "      <td>163.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>1,322,821</td>\n",
       "      <td>1,136,137</td>\n",
       "      <td>4.84%</td>\n",
       "      <td>152.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>1,317,728</td>\n",
       "      <td>1,133,837</td>\n",
       "      <td>4.83%</td>\n",
       "      <td>152.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>1,313,391</td>\n",
       "      <td>1,128,907</td>\n",
       "      <td>4.81%</td>\n",
       "      <td>151.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>932,470</td>\n",
       "      <td>3.97%</td>\n",
       "      <td>125.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>1,043,759</td>\n",
       "      <td>904,642</td>\n",
       "      <td>3.85%</td>\n",
       "      <td>121.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>994,154</td>\n",
       "      <td>870,665</td>\n",
       "      <td>3.71%</td>\n",
       "      <td>116.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>774,869</td>\n",
       "      <td>670,881</td>\n",
       "      <td>2.86%</td>\n",
       "      <td>90.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>751,396</td>\n",
       "      <td>650,302</td>\n",
       "      <td>2.77%</td>\n",
       "      <td>87.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>673,107</td>\n",
       "      <td>614,227</td>\n",
       "      <td>2.62%</td>\n",
       "      <td>82.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>493,167</td>\n",
       "      <td>412,612</td>\n",
       "      <td>1.76%</td>\n",
       "      <td>55.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>457,608</td>\n",
       "      <td>406,416</td>\n",
       "      <td>1.73%</td>\n",
       "      <td>54.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>393,722</td>\n",
       "      <td>358,863</td>\n",
       "      <td>1.53%</td>\n",
       "      <td>48.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>302,621</td>\n",
       "      <td>272,159</td>\n",
       "      <td>1.16%</td>\n",
       "      <td>36.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir-UT</td>\n",
       "      <td>227,927</td>\n",
       "      <td>199,917</td>\n",
       "      <td>0.85%</td>\n",
       "      <td>26.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>195,405</td>\n",
       "      <td>176,269</td>\n",
       "      <td>0.75%</td>\n",
       "      <td>23.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>-</td>\n",
       "      <td>82,604</td>\n",
       "      <td>0.35%</td>\n",
       "      <td>11.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>72,636</td>\n",
       "      <td>62,550</td>\n",
       "      <td>0.27%</td>\n",
       "      <td>8.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>45,635</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>6.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>-</td>\n",
       "      <td>44,238</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>5.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>42,697</td>\n",
       "      <td>38,785</td>\n",
       "      <td>0.17%</td>\n",
       "      <td>5.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>42,756</td>\n",
       "      <td>37,557</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>5.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>-</td>\n",
       "      <td>36,594</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>4.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>35,124</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>31,913</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>-</td>\n",
       "      <td>27,824</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>10,371</td>\n",
       "      <td>0.04%</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                      STATE GSDP at current price (19-20)  \\\n",
       "0     1                Maharashtra                             -   \n",
       "1     2                 Tamil Nadu                     2,364,514   \n",
       "2     3              Uttar Pradesh                     2,257,575   \n",
       "3     4                  Karnataka                     2,241,368   \n",
       "4     5                    Gujarat                             -   \n",
       "5     6                West Bengal                     1,554,992   \n",
       "6     7                  Rajasthan                     1,413,620   \n",
       "7     8             Madhya Pradesh                     1,322,821   \n",
       "8     9             Andhra Pradesh                     1,317,728   \n",
       "9    10                  Telangana                     1,313,391   \n",
       "10   11                     Kerala                             -   \n",
       "11   12                      Delhi                     1,043,759   \n",
       "12   13                    Haryana                       994,154   \n",
       "13   14                     Odisha                       774,869   \n",
       "14   15                      Bihar                       751,396   \n",
       "15   16                     Punjab                       673,107   \n",
       "16   17                      Assam                       493,167   \n",
       "17   18               Chhattisgarh                       457,608   \n",
       "18   19                  Jharkhand                       393,722   \n",
       "19   20                Uttarakhand                       302,621   \n",
       "20   21         Jammu & Kashmir-UT                       227,927   \n",
       "21   22           Himachal Pradesh                       195,405   \n",
       "22   23                        Goa                             -   \n",
       "23   24                    Tripura                        72,636   \n",
       "24   25                 Chandigarh                             -   \n",
       "25   26                 Puducherry                             -   \n",
       "26   27                  Meghalaya                        42,697   \n",
       "27   28                     Sikkim                        42,756   \n",
       "28   29                    Manipur                             -   \n",
       "29   30          Arunachal Pradesh                             -   \n",
       "30   31                   Nagaland                             -   \n",
       "31   32                    Mizoram                             -   \n",
       "32   33  Andaman & Nicobar Islands                             -   \n",
       "\n",
       "   GSDP at current price (18-19) Share (18-19) GDP($ billion)  \n",
       "0                      3,108,022        13.24%        417.163  \n",
       "1                      2,071,286         8.82%        278.011  \n",
       "2                      1,974,532         8.41%        265.024  \n",
       "3                      1,962,725         8.36%        263.440  \n",
       "4                      1,937,066         8.25%        259.996  \n",
       "5                      1,363,926         5.81%        183.068  \n",
       "6                      1,218,193         5.19%        163.507  \n",
       "7                      1,136,137         4.84%        152.494  \n",
       "8                      1,133,837         4.83%        152.185  \n",
       "9                      1,128,907         4.81%        151.523  \n",
       "10                       932,470         3.97%        125.157  \n",
       "11                       904,642         3.85%        121.422  \n",
       "12                       870,665         3.71%        116.862  \n",
       "13                       670,881         2.86%         90.047  \n",
       "14                       650,302         2.77%         87.284  \n",
       "15                       614,227         2.62%         82.442  \n",
       "16                       412,612         1.76%         55.381  \n",
       "17                       406,416         1.73%         54.550  \n",
       "18                       358,863         1.53%         48.167  \n",
       "19                       272,159         1.16%         36.530  \n",
       "20                       199,917         0.85%         26.833  \n",
       "21                       176,269         0.75%         23.659  \n",
       "22                        82,604         0.35%         11.087  \n",
       "23                        62,550         0.27%          8.396  \n",
       "24                        45,635         0.19%          6.125  \n",
       "25                        44,238         0.19%          5.938  \n",
       "26                        38,785         0.17%          5.206  \n",
       "27                        37,557         0.16%          5.041  \n",
       "28                        36,594         0.16%          4.912  \n",
       "29                        35,124         0.15%          4.714  \n",
       "30                        31,913         0.14%          4.283  \n",
       "31                        27,824         0.12%          3.735  \n",
       "32                        10,371         0.04%          1.392  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'RANK':Rank,'STATE':State,'GSDP at current price (19-20)':GSDP1,'GSDP at current price (18-19)':GSDP2,\n",
    "                'Share (18-19)':Share,'GDP($ billion)':GDP_billion})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45af41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6158eb",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "391d1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "907a83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d00b423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.get(\"https://github.com\")\n",
    "time.sleep(4)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "36424f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting explore button and clicking on it\n",
    "explore = driver.find_element(By.XPATH,'//ul[@class=\"d-lg-flex list-style-none\"]/li[3]/button').click()\n",
    "\n",
    "# selecting trending option\n",
    "trend_url = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "30305550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating emplty list \n",
    "Repository_title=[]\n",
    "Repository_description=[]\n",
    "Contributors_count=[]\n",
    "Language_used=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f669b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/h2/a'):\n",
    "        Repository_title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Repository_title.append(\"_\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]'):\n",
    "        Repository_description.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Repository_description.append(\"_\")\n",
    "    \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"position-relative container-lg p-responsive pt-6\"]/div/div[2]/article/div[2]/a[2]'):\n",
    "        Contributors_count.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Contributors_count.append(\"_\")\n",
    "    \n",
    "            \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//span[@itemprop=\"programmingLanguage\"]'):\n",
    "        Language_used.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Language_used.append(\"_\")\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build by xpath= //span[@class=\"d-inline-block mr-3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d148b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 23 25 22\n"
     ]
    }
   ],
   "source": [
    "print(len(Repository_title),len(Repository_description),len(Contributors_count),len(Language_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f7ddf700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>countribution</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>charlax / professional-programming</td>\n",
       "      <td>A collection of learning resources for curious...</td>\n",
       "      <td>2,787</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-S00N / I-S00N</td>\n",
       "      <td>Langchain-Chatchat（原Langchain-ChatGLM）基于 Langc...</td>\n",
       "      <td>1,620</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatchat-space / Langchain-Chatchat</td>\n",
       "      <td>A modern download manager that supports all pl...</td>\n",
       "      <td>3,940</td>\n",
       "      <td>Dart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GopeedLab / gopeed</td>\n",
       "      <td>《Hello 算法》：动画图解、一键运行的数据结构与算法教程，支持 Python, C++,...</td>\n",
       "      <td>739</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>krahets / hello-algo</td>\n",
       "      <td>🔥 🔥 🔥 现代化、开源的 Linux 服务器运维管理面板。</td>\n",
       "      <td>7,689</td>\n",
       "      <td>Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1Panel-dev / 1Panel</td>\n",
       "      <td>html with targeted manipulation zones</td>\n",
       "      <td>1,415</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kalabasa / htmz</td>\n",
       "      <td>基于大模型搭建的微信聊天机器人，同时支持微信、企业微信、公众号、飞书、钉钉接入，可选择GPT...</td>\n",
       "      <td>21</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zhayujie / chatgpt-on-wechat</td>\n",
       "      <td>Automate the process of making money online.</td>\n",
       "      <td>6,045</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FujiwaraChoki / MoneyPrinterV2</td>\n",
       "      <td>A cross-platform ChatGPT/Gemini UI (Web / PWA ...</td>\n",
       "      <td>65</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ChatGPTNextWeb / ChatGPT-Next-Web</td>\n",
       "      <td>Minimal, clean, code for the Byte Pair Encodin...</td>\n",
       "      <td>51,420</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>karpathy / minbpe</td>\n",
       "      <td>The OS for your personal finances</td>\n",
       "      <td>455</td>\n",
       "      <td>Ruby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>maybe-finance / maybe</td>\n",
       "      <td>Master programming by recreating your favorite...</td>\n",
       "      <td>1,757</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>codecrafters-io / build-your-own-x</td>\n",
       "      <td>Your GenAI Second Brain 🧠 A personal productiv...</td>\n",
       "      <td>23,396</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>QuivrHQ / quivr</td>\n",
       "      <td>An intuitive GUI for GLIGEN that uses ComfyUI ...</td>\n",
       "      <td>2,727</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mut-ex / gligen-gui</td>\n",
       "      <td>🔎 Hunt down social media accounts by username ...</td>\n",
       "      <td>71</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sherlock-project / sherlock</td>\n",
       "      <td>An open-source remote desktop, and alternative...</td>\n",
       "      <td>5,880</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rustdesk / rustdesk</td>\n",
       "      <td>OpenAI 接口管理 &amp; 分发系统，支持 Azure、Anthropic Claude、G...</td>\n",
       "      <td>6,285</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>songquanpeng / one-api</td>\n",
       "      <td>The official gpt4free repository | various col...</td>\n",
       "      <td>2,345</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LargeWorldModel / LWM</td>\n",
       "      <td>Master the command line, in one page</td>\n",
       "      <td>403</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xtekky / gpt4free</td>\n",
       "      <td>🤱🏻 Turn any webpage into a desktop app with Ru...</td>\n",
       "      <td>12,291</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jlevy / the-art-of-command-line</td>\n",
       "      <td>i茅台app自动预约，每日自动预约，支持docker一键部署</td>\n",
       "      <td>13,989</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tw93 / Pake</td>\n",
       "      <td>PyTorch code and models for V-JEPA self-superv...</td>\n",
       "      <td>3,091</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>oddfar / campus-imaotai</td>\n",
       "      <td>🤖 Lobe Chat - an open-source, high-performance...</td>\n",
       "      <td>801</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>facebookresearch / jepa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lobehub / lobe-chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Repository Title  \\\n",
       "0    charlax / professional-programming   \n",
       "1                       I-S00N / I-S00N   \n",
       "2   chatchat-space / Langchain-Chatchat   \n",
       "3                    GopeedLab / gopeed   \n",
       "4                  krahets / hello-algo   \n",
       "5                   1Panel-dev / 1Panel   \n",
       "6                       Kalabasa / htmz   \n",
       "7          zhayujie / chatgpt-on-wechat   \n",
       "8        FujiwaraChoki / MoneyPrinterV2   \n",
       "9     ChatGPTNextWeb / ChatGPT-Next-Web   \n",
       "10                    karpathy / minbpe   \n",
       "11                maybe-finance / maybe   \n",
       "12   codecrafters-io / build-your-own-x   \n",
       "13                      QuivrHQ / quivr   \n",
       "14                  mut-ex / gligen-gui   \n",
       "15          sherlock-project / sherlock   \n",
       "16                  rustdesk / rustdesk   \n",
       "17               songquanpeng / one-api   \n",
       "18                LargeWorldModel / LWM   \n",
       "19                    xtekky / gpt4free   \n",
       "20      jlevy / the-art-of-command-line   \n",
       "21                          tw93 / Pake   \n",
       "22              oddfar / campus-imaotai   \n",
       "23              facebookresearch / jepa   \n",
       "24                  lobehub / lobe-chat   \n",
       "\n",
       "                               Repository Description countribution  \\\n",
       "0   A collection of learning resources for curious...         2,787   \n",
       "1   Langchain-Chatchat（原Langchain-ChatGLM）基于 Langc...         1,620   \n",
       "2   A modern download manager that supports all pl...         3,940   \n",
       "3   《Hello 算法》：动画图解、一键运行的数据结构与算法教程，支持 Python, C++,...           739   \n",
       "4                      🔥 🔥 🔥 现代化、开源的 Linux 服务器运维管理面板。         7,689   \n",
       "5               html with targeted manipulation zones         1,415   \n",
       "6   基于大模型搭建的微信聊天机器人，同时支持微信、企业微信、公众号、飞书、钉钉接入，可选择GPT...            21   \n",
       "7        Automate the process of making money online.         6,045   \n",
       "8   A cross-platform ChatGPT/Gemini UI (Web / PWA ...            65   \n",
       "9   Minimal, clean, code for the Byte Pair Encodin...        51,420   \n",
       "10                  The OS for your personal finances           455   \n",
       "11  Master programming by recreating your favorite...         1,757   \n",
       "12  Your GenAI Second Brain 🧠 A personal productiv...        23,396   \n",
       "13  An intuitive GUI for GLIGEN that uses ComfyUI ...         2,727   \n",
       "14  🔎 Hunt down social media accounts by username ...            71   \n",
       "15  An open-source remote desktop, and alternative...         5,880   \n",
       "16  OpenAI 接口管理 & 分发系统，支持 Azure、Anthropic Claude、G...         6,285   \n",
       "17  The official gpt4free repository | various col...         2,345   \n",
       "18               Master the command line, in one page           403   \n",
       "19  🤱🏻 Turn any webpage into a desktop app with Ru...        12,291   \n",
       "20                     i茅台app自动预约，每日自动预约，支持docker一键部署        13,989   \n",
       "21  PyTorch code and models for V-JEPA self-superv...         3,091   \n",
       "22  🤖 Lobe Chat - an open-source, high-performance...           801   \n",
       "23                                                NaN           111   \n",
       "24                                                NaN         3,690   \n",
       "\n",
       "      Language  \n",
       "0       Python  \n",
       "1       Python  \n",
       "2         Dart  \n",
       "3         Java  \n",
       "4           Go  \n",
       "5   JavaScript  \n",
       "6       Python  \n",
       "7       Python  \n",
       "8   TypeScript  \n",
       "9       Python  \n",
       "10        Ruby  \n",
       "11  TypeScript  \n",
       "12  JavaScript  \n",
       "13      Python  \n",
       "14        Rust  \n",
       "15  JavaScript  \n",
       "16      Python  \n",
       "17      Python  \n",
       "18        Rust  \n",
       "19        Java  \n",
       "20      Python  \n",
       "21  TypeScript  \n",
       "22         NaN  \n",
       "23         NaN  \n",
       "24         NaN  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git={'Repository Title':Repository_title, 'Repository Description':Repository_description,\n",
    "                 'countribution' :Contributors_count,'Language':Language_used}\n",
    "\n",
    "df=pd.DataFrame({key:pd.Series(value) for key, value in git.items() })\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5bdbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17e92e3e",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cdcf9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "11588dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c68fd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.billboard.com/\")\n",
    "time.sleep(6)\n",
    "driver.maximize_window()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "11477fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on option button\n",
    "charts=driver.find_element(By.XPATH,'/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "charts.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1aa77e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on hot 100 button\n",
    "hot=driver.find_element(By.XPATH,'//div[@class=\"u-flex-grow-0 lrv-u-text-align-center\"]/span/a')\n",
    "hot.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "73be5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Blank List\n",
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Last_week_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_on_board=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef93f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping data of song names\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[1]/h3'):\n",
    "    Song_name.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of artist names\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[1]/span'):\n",
    "    Artist_name.append(i.text)\n",
    "    \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[4]/span'):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[5]/span'):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[6]/span'):\n",
    "    Weeks_on_board.append(i.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "654139aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Song_name),len( Artist_name),len( Last_week_rank),len(Peak_rank),len(Weeks_on_board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "462daadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SONG NAME</th>\n",
       "      <th>ARTIST NAME</th>\n",
       "      <th>LAST WEEK RANK</th>\n",
       "      <th>PEAK RANK</th>\n",
       "      <th>WEEK ON BOARD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lovin On Me</td>\n",
       "      <td>Jack Harlow</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lose Control</td>\n",
       "      <td>Teddy Swims</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beautiful Things</td>\n",
       "      <td>Benson Boone</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snooze</td>\n",
       "      <td>SZA</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Worth It</td>\n",
       "      <td>Offset &amp; Don Toliver</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Northern Attitude</td>\n",
       "      <td>Noah Kahan With Hozier</td>\n",
       "      <td>97</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Scared To Start</td>\n",
       "      <td>Michael Marcagi</td>\n",
       "      <td>-</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>First Love</td>\n",
       "      <td>Oscar Ortiz X Edgardo Nunez</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Where It Ends</td>\n",
       "      <td>Bailey Zimmerman</td>\n",
       "      <td>-</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            SONG NAME                  ARTIST NAME LAST WEEK RANK PEAK RANK  \\\n",
       "0         Lovin On Me                  Jack Harlow              2         1   \n",
       "1        Lose Control                  Teddy Swims              4         2   \n",
       "2    Beautiful Things                 Benson Boone              8         3   \n",
       "3        Cruel Summer                 Taylor Swift              3         1   \n",
       "4              Snooze                          SZA             10         2   \n",
       "..                ...                          ...            ...       ...   \n",
       "95           Worth It         Offset & Don Toliver            100        92   \n",
       "96  Northern Attitude       Noah Kahan With Hozier             97        37   \n",
       "97    Scared To Start              Michael Marcagi              -        98   \n",
       "98         First Love  Oscar Ortiz X Edgardo Nunez             91        91   \n",
       "99      Where It Ends             Bailey Zimmerman              -        32   \n",
       "\n",
       "   WEEK ON BOARD  \n",
       "0             13  \n",
       "1             26  \n",
       "2              3  \n",
       "3             40  \n",
       "4             60  \n",
       "..           ...  \n",
       "95             4  \n",
       "96            10  \n",
       "97             1  \n",
       "98             3  \n",
       "99             7  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'SONG NAME':Song_name,'ARTIST NAME':Artist_name,'LAST WEEK RANK':Last_week_rank,\n",
    "                 'PEAK RANK':Peak_rank,'WEEK ON BOARD':Weeks_on_board})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104a1b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d56b07a4",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d03f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bbd0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d94c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f09afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping book names data\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraping author names data\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author_name.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of volumes sold\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of publisher names\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping  data of genre\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[6]\"):\n",
    "    Genre.append(i.text)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02c54c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Book_name),len( Author_name),len(Volumes_sold),len(Publisher),len(Genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d6d7697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name            Author  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume sold        Publisher                        Genre  \n",
       "0    5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1    4,475,152       Bloomsbury           Children's Fiction  \n",
       "2    4,200,654       Bloomsbury           Children's Fiction  \n",
       "3    4,179,479       Bloomsbury           Children's Fiction  \n",
       "4    3,758,936     Random House              Romance & Sagas  \n",
       "..         ...              ...                          ...  \n",
       "95     807,311     Random House   General & Literary Fiction  \n",
       "96     794,201          Penguin        Food & Drink: General  \n",
       "97     792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98     791,507            Orion           Biography: General  \n",
       "99     791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "Novels = pd.DataFrame({})\n",
    "Novels['Book Name'] = Book_name\n",
    "Novels['Author'] = Author_name\n",
    "Novels['Volume sold'] = Volumes_sold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels['Genre'] = Genre\n",
    "Novels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53687dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f413d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4f379ee",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url =https://www.imdb.com/search/title/?title_type=tv_series&sort=num_votes,desc You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7abf4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adcf8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "969f3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage of mentioned url\n",
    "driver.get('https://www.imdb.com/search/title/?title_type=tv_series&sort=num_votes,desc')\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf8f1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url=[]\n",
    "url=driver.find_elements(By.XPATH,'//div[@class=\"sc-be6f1408-0 gVGktK\"]/div/a')\n",
    "for i in url:\n",
    "    title=i.text\n",
    "    page_url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "325aaa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2b11cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e093428",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# scraped data of Names\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"sc-be6f1408-0 gVGktK\"]/div/a/h3'):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraped data of Year span\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"sc-be6f1408-7 iUtHEN dli-title-metadata\"]/span[1]'):\n",
    "    Year_span.append(i.text)\n",
    "        \n",
    "        \n",
    "# scraped data of Ratings\n",
    "for i in driver.find_elements(By.XPATH,'//span[@class=\"sc-be6f1408-1 dbnleL\"]/div[1]/span'):\n",
    "    Ratings.append(i.text)\n",
    "            \n",
    "# scraped data of Votes\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"sc-f24f1c5c-2 ffDWee\"]/div[2]'):\n",
    "    Votes.append(i.text) \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "137bfd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50\n"
     ]
    }
   ],
   "source": [
    "print(len(Name),len(Year_span),len(Ratings),len(Votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d2a4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in page_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # scraped data of Run time\n",
    "    for i in driver.find_elements(By.XPATH,'//div[@class=\"sc-69e49b85-0 jqlHBQ\"]/ul/li[4]'):\n",
    "        Run_time.append(i.text)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34a49fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(Run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "905ada47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>YEAR OF SPAN</th>\n",
       "      <th>RATINGS</th>\n",
       "      <th>VOTES</th>\n",
       "      <th>RUN TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Game of Thrones</td>\n",
       "      <td>2011–2019</td>\n",
       "      <td>9.2\\n (2.3M)</td>\n",
       "      <td>Votes2,258,862</td>\n",
       "      <td>69h 49m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Breaking Bad</td>\n",
       "      <td>2008–2013</td>\n",
       "      <td>9.5\\n (2.1M)</td>\n",
       "      <td>Votes2,106,198</td>\n",
       "      <td>45m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. Stranger Things</td>\n",
       "      <td>2016–2025</td>\n",
       "      <td>8.7\\n (1.3M)</td>\n",
       "      <td>Votes1,317,843</td>\n",
       "      <td>51m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. Friends</td>\n",
       "      <td>1994–2004</td>\n",
       "      <td>8.9\\n (1.1M)</td>\n",
       "      <td>Votes1,077,203</td>\n",
       "      <td>88h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. The Walking Dead</td>\n",
       "      <td>2010–2022</td>\n",
       "      <td>8.1\\n (1.1M)</td>\n",
       "      <td>Votes1,070,251</td>\n",
       "      <td>44m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6. Sherlock</td>\n",
       "      <td>2010–2017</td>\n",
       "      <td>9.1\\n (988K)</td>\n",
       "      <td>Votes988,174</td>\n",
       "      <td>1h 28m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7. The Big Bang Theory</td>\n",
       "      <td>2007–2019</td>\n",
       "      <td>8.2\\n (860K)</td>\n",
       "      <td>Votes859,960</td>\n",
       "      <td>95h 2m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8. Dexter</td>\n",
       "      <td>2006–2013</td>\n",
       "      <td>8.7\\n (759K)</td>\n",
       "      <td>Votes758,779</td>\n",
       "      <td>55m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9. How I Met Your Mother</td>\n",
       "      <td>2005–2014</td>\n",
       "      <td>8.3\\n (723K)</td>\n",
       "      <td>Votes723,291</td>\n",
       "      <td>76h 16m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10. The Office</td>\n",
       "      <td>2005–2013</td>\n",
       "      <td>9.0\\n (698K)</td>\n",
       "      <td>Votes698,277</td>\n",
       "      <td>22m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11. True Detective</td>\n",
       "      <td>2014–</td>\n",
       "      <td>8.9\\n (641K)</td>\n",
       "      <td>Votes640,672</td>\n",
       "      <td>55m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12. Peaky Blinders</td>\n",
       "      <td>2013–2022</td>\n",
       "      <td>8.8\\n (636K)</td>\n",
       "      <td>Votes636,091</td>\n",
       "      <td>35h 9m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13. Better Call Saul</td>\n",
       "      <td>2015–2022</td>\n",
       "      <td>9.0\\n (635K)</td>\n",
       "      <td>Votes635,258</td>\n",
       "      <td>52h 34m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14. The Boys</td>\n",
       "      <td>2019–</td>\n",
       "      <td>8.7\\n (633K)</td>\n",
       "      <td>Votes633,223</td>\n",
       "      <td>1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15. Black Mirror</td>\n",
       "      <td>2011–</td>\n",
       "      <td>8.7\\n (632K)</td>\n",
       "      <td>Votes632,128</td>\n",
       "      <td>1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16. Rick and Morty</td>\n",
       "      <td>2013–</td>\n",
       "      <td>9.1\\n (594K)</td>\n",
       "      <td>Votes593,588</td>\n",
       "      <td>23m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17. Lost</td>\n",
       "      <td>2004–2010</td>\n",
       "      <td>8.3\\n (590K)</td>\n",
       "      <td>Votes589,612</td>\n",
       "      <td>90h 45m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18. The Mandalorian</td>\n",
       "      <td>2019–</td>\n",
       "      <td>8.7\\n (580K)</td>\n",
       "      <td>Votes579,519</td>\n",
       "      <td>40m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19. Vikings</td>\n",
       "      <td>2013–2020</td>\n",
       "      <td>8.5\\n (575K)</td>\n",
       "      <td>Votes575,171</td>\n",
       "      <td>44m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20. Prison Break</td>\n",
       "      <td>2005–2017</td>\n",
       "      <td>8.3\\n (574K)</td>\n",
       "      <td>Votes574,230</td>\n",
       "      <td>44m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21. The Witcher</td>\n",
       "      <td>2019–</td>\n",
       "      <td>8.0\\n (564K)</td>\n",
       "      <td>Votes564,451</td>\n",
       "      <td>1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22. Squid Game</td>\n",
       "      <td>2021–</td>\n",
       "      <td>8.0\\n (530K)</td>\n",
       "      <td>Votes530,146</td>\n",
       "      <td>55m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23. Westworld</td>\n",
       "      <td>2016–2022</td>\n",
       "      <td>8.5\\n (529K)</td>\n",
       "      <td>Votes529,210</td>\n",
       "      <td>1h 2m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24. House of Cards</td>\n",
       "      <td>2013–2018</td>\n",
       "      <td>8.6\\n (528K)</td>\n",
       "      <td>Votes527,901</td>\n",
       "      <td>63h 24m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25. Money Heist</td>\n",
       "      <td>2017–2021</td>\n",
       "      <td>8.2\\n (526K)</td>\n",
       "      <td>Votes525,824</td>\n",
       "      <td>1h 10m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26. House</td>\n",
       "      <td>2004–2012</td>\n",
       "      <td>8.7\\n (504K)</td>\n",
       "      <td>Votes503,988</td>\n",
       "      <td>129h 48m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27. The Last of Us</td>\n",
       "      <td>2023–</td>\n",
       "      <td>8.8\\n (501K)</td>\n",
       "      <td>Votes500,528</td>\n",
       "      <td>50m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28. Attack on Titan</td>\n",
       "      <td>2013–2023</td>\n",
       "      <td>9.1\\n (497K)</td>\n",
       "      <td>Votes497,463</td>\n",
       "      <td>24m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29. Supernatural</td>\n",
       "      <td>2005–2020</td>\n",
       "      <td>8.4\\n (478K)</td>\n",
       "      <td>Votes478,425</td>\n",
       "      <td>44m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30. Modern Family</td>\n",
       "      <td>2009–2020</td>\n",
       "      <td>8.5\\n (476K)</td>\n",
       "      <td>Votes476,317</td>\n",
       "      <td>89h 38m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31. Suits</td>\n",
       "      <td>2011–2019</td>\n",
       "      <td>8.4\\n (471K)</td>\n",
       "      <td>Votes471,133</td>\n",
       "      <td>44m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32. Daredevil</td>\n",
       "      <td>2015–2018</td>\n",
       "      <td>8.6\\n (471K)</td>\n",
       "      <td>Votes470,775</td>\n",
       "      <td>54m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33. Narcos</td>\n",
       "      <td>2015–2017</td>\n",
       "      <td>8.8\\n (465K)</td>\n",
       "      <td>Votes464,989</td>\n",
       "      <td>49m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34. The Sopranos</td>\n",
       "      <td>1999–2007</td>\n",
       "      <td>9.2\\n (462K)</td>\n",
       "      <td>Votes461,711</td>\n",
       "      <td>55m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35. Arrow</td>\n",
       "      <td>2012–2020</td>\n",
       "      <td>7.5\\n (445K)</td>\n",
       "      <td>Votes444,552</td>\n",
       "      <td>42m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36. Dark</td>\n",
       "      <td>2017–2020</td>\n",
       "      <td>8.7\\n (437K)</td>\n",
       "      <td>Votes436,943</td>\n",
       "      <td>24h 15m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37. The Simpsons</td>\n",
       "      <td>1989–</td>\n",
       "      <td>8.7\\n (433K)</td>\n",
       "      <td>Votes432,621</td>\n",
       "      <td>22m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38. Fargo</td>\n",
       "      <td>2014–2024</td>\n",
       "      <td>8.9\\n (415K)</td>\n",
       "      <td>Votes415,410</td>\n",
       "      <td>53m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39. Mr. Robot</td>\n",
       "      <td>2015–2019</td>\n",
       "      <td>8.5\\n (415K)</td>\n",
       "      <td>Votes414,742</td>\n",
       "      <td>49m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40. Loki</td>\n",
       "      <td>2021–2023</td>\n",
       "      <td>8.2\\n (406K)</td>\n",
       "      <td>Votes405,895</td>\n",
       "      <td>9h 46m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41. South Park</td>\n",
       "      <td>1997–</td>\n",
       "      <td>8.7\\n (403K)</td>\n",
       "      <td>Votes402,986</td>\n",
       "      <td>22m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42. The Wire</td>\n",
       "      <td>2002–2008</td>\n",
       "      <td>9.3\\n (372K)</td>\n",
       "      <td>Votes372,498</td>\n",
       "      <td>59m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43. Death Note</td>\n",
       "      <td>2006–2007</td>\n",
       "      <td>8.9\\n (372K)</td>\n",
       "      <td>Votes371,915</td>\n",
       "      <td>24m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44. The Flash</td>\n",
       "      <td>2014–2023</td>\n",
       "      <td>7.5\\n (367K)</td>\n",
       "      <td>Votes366,714</td>\n",
       "      <td>43m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45. House of the Dragon</td>\n",
       "      <td>2022–</td>\n",
       "      <td>8.4\\n (363K)</td>\n",
       "      <td>Votes362,865</td>\n",
       "      <td>50m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46. Family Guy</td>\n",
       "      <td>1999–2025</td>\n",
       "      <td>8.2\\n (362K)</td>\n",
       "      <td>Votes361,626</td>\n",
       "      <td>22m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47. Homeland</td>\n",
       "      <td>2011–2020</td>\n",
       "      <td>8.3\\n (359K)</td>\n",
       "      <td>Votes359,004</td>\n",
       "      <td>55m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48. Avatar: The Last Airbender</td>\n",
       "      <td>2005–2008</td>\n",
       "      <td>9.3\\n (357K)</td>\n",
       "      <td>Votes356,569</td>\n",
       "      <td>23m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49. Brooklyn Nine-Nine</td>\n",
       "      <td>2013–2021</td>\n",
       "      <td>8.4\\n (356K)</td>\n",
       "      <td>Votes355,568</td>\n",
       "      <td>22m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50. Wednesday</td>\n",
       "      <td>2022–</td>\n",
       "      <td>8.1\\n (352K)</td>\n",
       "      <td>Votes352,466</td>\n",
       "      <td>45m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              NAME YEAR OF SPAN       RATINGS           VOTES  \\\n",
       "0               1. Game of Thrones    2011–2019  9.2\\n (2.3M)  Votes2,258,862   \n",
       "1                  2. Breaking Bad    2008–2013  9.5\\n (2.1M)  Votes2,106,198   \n",
       "2               3. Stranger Things    2016–2025  8.7\\n (1.3M)  Votes1,317,843   \n",
       "3                       4. Friends    1994–2004  8.9\\n (1.1M)  Votes1,077,203   \n",
       "4              5. The Walking Dead    2010–2022  8.1\\n (1.1M)  Votes1,070,251   \n",
       "5                      6. Sherlock    2010–2017  9.1\\n (988K)    Votes988,174   \n",
       "6           7. The Big Bang Theory    2007–2019  8.2\\n (860K)    Votes859,960   \n",
       "7                        8. Dexter    2006–2013  8.7\\n (759K)    Votes758,779   \n",
       "8         9. How I Met Your Mother    2005–2014  8.3\\n (723K)    Votes723,291   \n",
       "9                   10. The Office    2005–2013  9.0\\n (698K)    Votes698,277   \n",
       "10              11. True Detective        2014–  8.9\\n (641K)    Votes640,672   \n",
       "11              12. Peaky Blinders    2013–2022  8.8\\n (636K)    Votes636,091   \n",
       "12            13. Better Call Saul    2015–2022  9.0\\n (635K)    Votes635,258   \n",
       "13                    14. The Boys        2019–  8.7\\n (633K)    Votes633,223   \n",
       "14                15. Black Mirror        2011–  8.7\\n (632K)    Votes632,128   \n",
       "15              16. Rick and Morty        2013–  9.1\\n (594K)    Votes593,588   \n",
       "16                        17. Lost    2004–2010  8.3\\n (590K)    Votes589,612   \n",
       "17             18. The Mandalorian        2019–  8.7\\n (580K)    Votes579,519   \n",
       "18                     19. Vikings    2013–2020  8.5\\n (575K)    Votes575,171   \n",
       "19                20. Prison Break    2005–2017  8.3\\n (574K)    Votes574,230   \n",
       "20                 21. The Witcher        2019–  8.0\\n (564K)    Votes564,451   \n",
       "21                  22. Squid Game        2021–  8.0\\n (530K)    Votes530,146   \n",
       "22                   23. Westworld    2016–2022  8.5\\n (529K)    Votes529,210   \n",
       "23              24. House of Cards    2013–2018  8.6\\n (528K)    Votes527,901   \n",
       "24                 25. Money Heist    2017–2021  8.2\\n (526K)    Votes525,824   \n",
       "25                       26. House    2004–2012  8.7\\n (504K)    Votes503,988   \n",
       "26              27. The Last of Us        2023–  8.8\\n (501K)    Votes500,528   \n",
       "27             28. Attack on Titan    2013–2023  9.1\\n (497K)    Votes497,463   \n",
       "28                29. Supernatural    2005–2020  8.4\\n (478K)    Votes478,425   \n",
       "29               30. Modern Family    2009–2020  8.5\\n (476K)    Votes476,317   \n",
       "30                       31. Suits    2011–2019  8.4\\n (471K)    Votes471,133   \n",
       "31                   32. Daredevil    2015–2018  8.6\\n (471K)    Votes470,775   \n",
       "32                      33. Narcos    2015–2017  8.8\\n (465K)    Votes464,989   \n",
       "33                34. The Sopranos    1999–2007  9.2\\n (462K)    Votes461,711   \n",
       "34                       35. Arrow    2012–2020  7.5\\n (445K)    Votes444,552   \n",
       "35                        36. Dark    2017–2020  8.7\\n (437K)    Votes436,943   \n",
       "36                37. The Simpsons        1989–  8.7\\n (433K)    Votes432,621   \n",
       "37                       38. Fargo    2014–2024  8.9\\n (415K)    Votes415,410   \n",
       "38                   39. Mr. Robot    2015–2019  8.5\\n (415K)    Votes414,742   \n",
       "39                        40. Loki    2021–2023  8.2\\n (406K)    Votes405,895   \n",
       "40                  41. South Park        1997–  8.7\\n (403K)    Votes402,986   \n",
       "41                    42. The Wire    2002–2008  9.3\\n (372K)    Votes372,498   \n",
       "42                  43. Death Note    2006–2007  8.9\\n (372K)    Votes371,915   \n",
       "43                   44. The Flash    2014–2023  7.5\\n (367K)    Votes366,714   \n",
       "44         45. House of the Dragon        2022–  8.4\\n (363K)    Votes362,865   \n",
       "45                  46. Family Guy    1999–2025  8.2\\n (362K)    Votes361,626   \n",
       "46                    47. Homeland    2011–2020  8.3\\n (359K)    Votes359,004   \n",
       "47  48. Avatar: The Last Airbender    2005–2008  9.3\\n (357K)    Votes356,569   \n",
       "48          49. Brooklyn Nine-Nine    2013–2021  8.4\\n (356K)    Votes355,568   \n",
       "49                   50. Wednesday        2022–  8.1\\n (352K)    Votes352,466   \n",
       "\n",
       "    RUN TIME  \n",
       "0    69h 49m  \n",
       "1        45m  \n",
       "2        51m  \n",
       "3        88h  \n",
       "4        44m  \n",
       "5     1h 28m  \n",
       "6     95h 2m  \n",
       "7        55m  \n",
       "8    76h 16m  \n",
       "9        22m  \n",
       "10       55m  \n",
       "11    35h 9m  \n",
       "12   52h 34m  \n",
       "13        1h  \n",
       "14        1h  \n",
       "15       23m  \n",
       "16   90h 45m  \n",
       "17       40m  \n",
       "18       44m  \n",
       "19       44m  \n",
       "20        1h  \n",
       "21       55m  \n",
       "22     1h 2m  \n",
       "23   63h 24m  \n",
       "24    1h 10m  \n",
       "25  129h 48m  \n",
       "26       50m  \n",
       "27       24m  \n",
       "28       44m  \n",
       "29   89h 38m  \n",
       "30       44m  \n",
       "31       54m  \n",
       "32       49m  \n",
       "33       55m  \n",
       "34       42m  \n",
       "35   24h 15m  \n",
       "36       22m  \n",
       "37       53m  \n",
       "38       49m  \n",
       "39    9h 46m  \n",
       "40       22m  \n",
       "41       59m  \n",
       "42       24m  \n",
       "43       43m  \n",
       "44       50m  \n",
       "45       22m  \n",
       "46       55m  \n",
       "47       23m  \n",
       "48       22m  \n",
       "49       45m  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'NAME':Name,'YEAR OF SPAN':Year_span,'RATINGS':Ratings,'VOTES':Votes,'RUN TIME':Run_time})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b47455",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "    Url = https://archive.ics.uci.edu/ You\n",
    "    have to find the following details:\n",
    "    A) Dataset name\n",
    "    B) Data type\n",
    "    C) Task\n",
    "    D) Attribute type\n",
    "    E) No of instances\n",
    "    F) No of attribute G) Year\n",
    "     Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ec41648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importing selenium web driver\n",
    "from selenium import webdriver\n",
    "\n",
    "# importing required exceptions to handle\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing request\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cd77575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8763129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage of mentioned url\n",
    "driver.get(\" https://archive.ics.uci.edu/\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25c788e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(By.XPATH,'//div[@class=\"flex flex-wrap justify-center gap-5\"]/a[1]')\n",
    "page_url = viewall_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a893d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching  expand all datasets\n",
    "expand_all = driver.find_element(By.XPATH,'//label[@class=\"btn-primary btn-sm btn flex gap-2 rounded-full\"]/div[2]/span[2]')\n",
    "expand_all.click()\n",
    "# list_url = view_list.get_attribute(\"href\")\n",
    "# driver.get(list_url)\n",
    "time.sleep(3)\n",
    "\n",
    "#button xpath= //div[@class=\"btn-group\"]/button[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69d2c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd8a8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraped name \n",
    "for i in driver.find_elements(By.XPATH,'//h2[@class=\"truncate text-primary\"]/a'):\n",
    "    Dataset_name.append(i.text)\n",
    "    \n",
    "# scraped data type\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[2]/span'):\n",
    "    Data_type.append(i.text)\n",
    "    \n",
    "# scraped task\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[1]/span'):\n",
    "    Task.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped attribute type\n",
    "for i in driver.find_elements(By.XPATH,'//table[@class=\"col-span-full my-2 table sm:col-start-2\"]/tbody/tr/td[2]'):\n",
    "    Attribute_type.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped no of instance\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[3]/span'):\n",
    "    No_of_instances.append(i.text)\n",
    "    \n",
    "# scraped no of attribute\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[4]/span'):\n",
    "    No_of_attributes.append(i.text)\n",
    "    \n",
    "# scraped year\n",
    "for i in driver.find_elements(By.XPATH,'//table[@class=\"col-span-full my-2 table sm:col-start-2\"]/tbody/tr/td[3]'):\n",
    "    Year.append(i.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2eec6dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Dataset_name),len(Data_type),len(Task),len(Attribute_type),len(No_of_instances),len(No_of_attributes),len(Year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdf730f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Features</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>16 Features</td>\n",
       "      <td>9/14/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Features</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>3.81K Instances</td>\n",
       "      <td>7 Features</td>\n",
       "      <td>10/6/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Features</td>\n",
       "      <td>5/1/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Raisin</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>900 Instances</td>\n",
       "      <td>8 Features</td>\n",
       "      <td>8/14/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>178 Instances</td>\n",
       "      <td>13 Features</td>\n",
       "      <td>7/1/1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>569 Instances</td>\n",
       "      <td>30 Features</td>\n",
       "      <td>11/1/1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wine Quality</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>4.9K Instances</td>\n",
       "      <td>12 Features</td>\n",
       "      <td>10/7/2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>1 Instances</td>\n",
       "      <td>20 Features</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Data Name                 Data Type   \\\n",
       "0                                  Iris                    Tabular   \n",
       "1                      Dry Bean Dataset               Multivariate   \n",
       "2                         Heart Disease               Multivariate   \n",
       "3            Rice (Cammeo and Osmancik)               Multivariate   \n",
       "4                                 Adult               Multivariate   \n",
       "5                                Raisin               Multivariate   \n",
       "6                                  Wine                    Tabular   \n",
       "7  Breast Cancer Wisconsin (Diagnostic)               Multivariate   \n",
       "8                          Wine Quality               Multivariate   \n",
       "9                              Diabetes  Multivariate, Time-Series   \n",
       "\n",
       "                        Task              Attribute Type    No of Instance   \\\n",
       "0              Classification                        Real     150 Instances   \n",
       "1              Classification               Integer, Real  13.61K Instances   \n",
       "2              Classification  Categorical, Integer, Real     303 Instances   \n",
       "3              Classification                        Real   3.81K Instances   \n",
       "4              Classification        Categorical, Integer  48.84K Instances   \n",
       "5              Classification               Real, Integer     900 Instances   \n",
       "6              Classification               Integer, Real     178 Instances   \n",
       "7              Classification                        Real     569 Instances   \n",
       "8  Classification, Regression                        Real    4.9K Instances   \n",
       "9              Classification        Categorical, Integer       1 Instances   \n",
       "\n",
       "  No of Attributes       Year   \n",
       "0        4 Features   7/1/1988  \n",
       "1       16 Features  9/14/2020  \n",
       "2       13 Features   7/1/1988  \n",
       "3        7 Features  10/6/2019  \n",
       "4       14 Features   5/1/1996  \n",
       "5        8 Features  8/14/2023  \n",
       "6       13 Features   7/1/1991  \n",
       "7       30 Features  11/1/1995  \n",
       "8       12 Features  10/7/2009  \n",
       "9       20 Features        N/A  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML = pd.DataFrame({'Data Name': Dataset_name, 'Data Type ': Data_type,'Task ':Task ,'Attribute Type ':Attribute_type ,\n",
    "'No of Instance ': No_of_instances,'No of Attributes ':No_of_attributes, 'Year ': Year}) \n",
    "\n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c18596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590b00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0809b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
